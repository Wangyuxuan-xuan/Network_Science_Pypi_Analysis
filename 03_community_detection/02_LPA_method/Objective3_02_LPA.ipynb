{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa86d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING LPA ANALYSIS ---\n",
      "Loading graph from E:\\Network Science Project\\pypi_dag\\edges.csv...\n",
      "Graph loaded. Nodes: 397798, Edges: 1819937\n",
      "Running Label Propagation Algorithm (LPA)...\n",
      "Calculating Modularity (Q)...\n",
      "LPA completed. Modularity Score (Q): 0.1140\n",
      "Analyzing structure...\n",
      "Calculating PageRank for Hub identification...\n",
      "\n",
      "--- TOP 10 LPA STACKS ---\n",
      "Stack #1 (ID: 0): Size=343387 (86.32%) -> Hubs: numpy, typing-extensions, requests, colorama, six\n",
      "Stack #2 (ID: 78): Size=18049 (4.54%) -> Hubs: odoo, python-stdnum, odoo14-addon-ssi-transaction-mixin, openupgradelib, odoo14-addon-ssi-master-data-mixin\n",
      "Stack #3 (ID: 2): Size=6265 (1.57%) -> Hubs: django, djangorestframework, wagtail, django-cms, django-model-utils\n",
      "Stack #4 (ID: 13): Size=1765 (0.44%) -> Hubs: typeguard, publication, jsii, constructs, aws-cdk-lib\n",
      "Stack #5 (ID: 54): Size=748 (0.19%) -> Hubs: python-sdk-remote, octodns, shipyard-templates, ckantools, simplematch\n",
      "Stack #6 (ID: 5): Size=666 (0.17%) -> Hubs: zope-interface, zope-event, zope-component, zope, zope-schema\n",
      "Stack #7 (ID: 193): Size=633 (0.16%) -> Hubs: alibabacloud-tea-util, alibabacloud-tea-openapi, alibabacloud-endpoint-util, alibabacloud-openapi-util, alibabacloud-credentials\n",
      "Stack #8 (ID: 802): Size=463 (0.12%) -> Hubs: winrt-runtime, winrt-windows-ui-xaml, winui3-microsoft-ui-xaml, winrt-windows-ui-composition, winrt-windows-ui-xaml-media\n",
      "Stack #9 (ID: 283): Size=411 (0.10%) -> Hubs: alibabacloud-tea-util-py2, alibabacloud-tea-openapi-py2, alibabacloud-endpoint-util-py2, alibabacloud-openapi-util-py2, alibabacloud-tea-fileform-py2\n",
      "Stack #10 (ID: 284): Size=384 (0.10%) -> Hubs: alibabacloud-rpc-util, antchain-alipay-util, antchain-aitechguard, antchain-ak-2d01ff274c3448c2b919937512c80f91, antchain-ak-a3144b2bb72d485dba4479e94be6945d\n",
      "\n",
      "--- EXPORTING RESULTS ---\n",
      "Summary saved to E:\\Network Science Project\\02_LPA_method\\RealData\\analysis_summary_lpa.txt\n",
      "  - Extracting Top 200 PageRank nodes...\n",
      "  - Exporting GEXF: 1_pagerank_core_lpa.gexf...\n",
      "    Done. Saved to E:\\Network Science Project\\02_LPA_method\\RealData\\1_pagerank_core_lpa.gexf\n",
      "  - Building Abstract Community Graph...\n",
      "  - Exporting GEXF: 2_abstract_community_network_lpa.gexf...\n",
      "    Done. Saved to E:\\Network Science Project\\02_LPA_method\\RealData\\2_abstract_community_network_lpa.gexf\n",
      "\n",
      " LPA Analysis COMPLETE!\n",
      "Files saved in: E:\\Network Science Project\\02_LPA_method\\RealData\n"
     ]
    }
   ],
   "source": [
    "# (Objective3) 02_LPA_RealData\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm  # Key: correctly import community algorithms module\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from community import community_louvain  # Only used to compute modularity Q\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# Input paths\n",
    "DATA_PATH = r\"E:\\Network Science Project\\pypi_dag\"\n",
    "EDGES_FILE = os.path.join(DATA_PATH, \"edges.csv\")\n",
    "\n",
    "# Output path (automatically create LPA_Analysis folder)\n",
    "OUTPUT_ROOT_PATH = r\"E:\\Network Science Project\\02_LPA_method\\RealData\"\n",
    "os.makedirs(OUTPUT_ROOT_PATH, exist_ok=True)\n",
    "\n",
    "# Visualization parameters\n",
    "PAGERANK_SAMPLE_NODES = 200  # File 1: number of core subgraph nodes\n",
    "TOP_COMMUNITIES_SAMPLE = 5   # File 2: number of top communities\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA LOADING & LPA CORE\n",
    "# ==========================================\n",
    "\n",
    "def load_graph(edges_file: str) -> nx.DiGraph:\n",
    "    \"\"\"Load graph data (parameter issue fixed)\"\"\"\n",
    "    print(f\"Loading graph from {edges_file}...\")\n",
    "    try:\n",
    "        df_edges = pd.read_csv(edges_file)\n",
    "        G = nx.from_pandas_edgelist(\n",
    "            df_edges,\n",
    "            source='source',\n",
    "            target='target',\n",
    "            create_using=nx.DiGraph()  # Fix 1: use create_using instead of create_graph\n",
    "        )\n",
    "        print(f\"Graph loaded. Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
    "        return G\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_lpa_community_detection(G: nx.DiGraph) -> tuple:\n",
    "    \"\"\"Run LPA algorithm and compute modularity\"\"\"\n",
    "    print(\"Running Label Propagation Algorithm (LPA)...\")\n",
    "    \n",
    "    # LPA usually performs best on undirected graphs\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    # Fix 2: use the correct module path to call LPA\n",
    "    # label_propagation_communities returns a generator of node sets\n",
    "    lpa_gen = nx_comm.label_propagation_communities(G_undirected)\n",
    "    \n",
    "    # Convert set generator into {node: community_id} dictionary\n",
    "    partition = {}\n",
    "    for idx, community_nodes in enumerate(lpa_gen):\n",
    "        for node in community_nodes:\n",
    "            partition[node] = idx\n",
    "            \n",
    "    # Compute modularity (using Louvain library's evaluation function for convenience)\n",
    "    # Note: modularity calculation may consume a lot of memory; comment out if OOM\n",
    "    print(\"Calculating Modularity (Q)...\")\n",
    "    try:\n",
    "        modularity_score = community_louvain.modularity(partition, G_undirected)\n",
    "        print(f\"LPA completed. Modularity Score (Q): {modularity_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate modularity (possibly OOM). Setting to 0. Error: {e}\")\n",
    "        modularity_score = 0.0\n",
    "        \n",
    "    return partition, modularity_score\n",
    "\n",
    "def analyze_structure(G: nx.DiGraph, partition: dict, modularity_score: float) -> dict:\n",
    "    \"\"\"Analyze results, extract Top 10, and compute hubs\"\"\"\n",
    "    print(\"Analyzing structure...\")\n",
    "    \n",
    "    community_map = defaultdict(list)\n",
    "    for node, comm_id in partition.items():\n",
    "        community_map[comm_id].append(node)\n",
    "        \n",
    "    # Sort by community size\n",
    "    sorted_communities = sorted(community_map.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    # Compute PageRank for hub identification\n",
    "    print(\"Calculating PageRank for Hub identification...\")\n",
    "    pagerank_scores = nx.pagerank(G, alpha=0.85)\n",
    "    \n",
    "    # Build Top 10 summary\n",
    "    top_10_summary = []\n",
    "    total_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Also build hubs dictionary for all communities (used for file 3 visualization)\n",
    "    community_hubs = {}\n",
    "    \n",
    "    for comm_id, nodes in sorted_communities:\n",
    "        # Get top 5 hubs for this community\n",
    "        node_pr = [(n, pagerank_scores.get(n, 0)) for n in nodes]\n",
    "        top_hubs = sorted(node_pr, key=lambda x: x[1], reverse=True)[:5]\n",
    "        hubs_str = ', '.join([n for n, pr in top_hubs])\n",
    "        community_hubs[comm_id] = hubs_str\n",
    "        \n",
    "    # Only extract Top 10 for reporting\n",
    "    print(\"\\n--- TOP 10 LPA STACKS ---\")\n",
    "    for rank, (comm_id, nodes) in enumerate(sorted_communities[:10]):\n",
    "        size = len(nodes)\n",
    "        share = (size / total_nodes) * 100\n",
    "        \n",
    "        stack_info = {\n",
    "            'Rank': rank + 1,\n",
    "            'ID': comm_id,\n",
    "            'Size': size,\n",
    "            'Share': share,\n",
    "            'Core Hubs': community_hubs[comm_id]\n",
    "        }\n",
    "        top_10_summary.append(stack_info)\n",
    "        print(f\"Stack #{rank+1} (ID: {comm_id}): Size={size} ({share:.2f}%) -> Hubs: {stack_info['Core Hubs']}\")\n",
    "\n",
    "    return {\n",
    "        \"graph_name\": \"Real PyPI (LPA)\",\n",
    "        \"modularity\": modularity_score,\n",
    "        \"num_communities\": len(sorted_communities),\n",
    "        \"partition\": partition,\n",
    "        \"top_communities_summary\": top_10_summary,\n",
    "        \"community_hubs\": community_hubs,\n",
    "        \"pagerank_scores\": pagerank_scores,\n",
    "        \"total_nodes\": total_nodes,\n",
    "        \"full_sorted_communities\": sorted_communities  # Used for file 2\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# 3. VISUALIZATION EXPORT FUNCTIONS (FULL VERSION)\n",
    "# ==========================================\n",
    "\n",
    "def export_for_visualization(G: nx.DiGraph, partition: dict, output_path: str, filename: str):\n",
    "    \"\"\"Generic GEXF export function\"\"\"\n",
    "    file_path = os.path.join(output_path, filename)\n",
    "    print(f\"  - Exporting GEXF: {filename}...\")\n",
    "    \n",
    "    G_export = G.copy()\n",
    "    nx.set_node_attributes(G_export, partition, name='CommunityID')\n",
    "    \n",
    "    # Simple edge type labeling\n",
    "    edge_type = {}\n",
    "    for u, v in G_export.edges():\n",
    "        if partition.get(u) == partition.get(v):\n",
    "            edge_type[(u, v)] = 'Internal'\n",
    "        else:\n",
    "            edge_type[(u, v)] = 'Inter-Community'\n",
    "    nx.set_edge_attributes(G_export, edge_type, name='LinkType')\n",
    "    \n",
    "    nx.write_gexf(G_export, file_path)\n",
    "    print(f\"    Done. Saved to {file_path}\")\n",
    "\n",
    "def extract_core_subgraph(G, partition, pr_scores, num_nodes):\n",
    "    \"\"\"Strategy 1: extract PageRank-based core subgraph\"\"\"\n",
    "    print(f\"  - Extracting Top {num_nodes} PageRank nodes...\")\n",
    "    top_nodes = sorted(pr_scores, key=pr_scores.get, reverse=True)[:num_nodes]\n",
    "    subgraph = G.subgraph(top_nodes).copy()\n",
    "    # Add PageRank attribute to nodes for easier sizing in Gephi\n",
    "    nx.set_node_attributes(subgraph, {n: pr_scores[n] for n in top_nodes}, name='PageRank')\n",
    "    sub_partition = {n: partition[n] for n in top_nodes if n in partition}\n",
    "    return subgraph, sub_partition\n",
    "\n",
    "def build_community_graph(G, partition, community_hubs):\n",
    "    \"\"\"Strategy 2: build abstract community network (macro view)\"\"\"\n",
    "    print(\"  - Building Abstract Community Graph...\")\n",
    "    G_comm = nx.DiGraph()\n",
    "    comm_sizes = defaultdict(int)\n",
    "    inter_edges = defaultdict(int)\n",
    "    \n",
    "    for n, comm_id in partition.items():\n",
    "        comm_sizes[comm_id] += 1\n",
    "        \n",
    "    for u, v in G.edges():\n",
    "        c_u, c_v = partition.get(u), partition.get(v)\n",
    "        if c_u is not None and c_v is not None and c_u != c_v:\n",
    "            inter_edges[(c_u, c_v)] += 1\n",
    "            \n",
    "    # Add nodes\n",
    "    for comm_id, size in comm_sizes.items():\n",
    "        G_comm.add_node(\n",
    "            comm_id,\n",
    "            size=size,\n",
    "            label=f\"Comm {comm_id}\",\n",
    "            Core_Hubs=community_hubs.get(comm_id, \"N/A\")\n",
    "        )\n",
    "        \n",
    "    # Add edges\n",
    "    for (u, v), w in inter_edges.items():\n",
    "        G_comm.add_edge(u, v, weight=w)\n",
    "        \n",
    "    return G_comm\n",
    "\n",
    "def save_txt_summary(results, filepath):\n",
    "    \"\"\"Save text summary\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"ANALYSIS SUMMARY (LPA Algorithm)\\n\")\n",
    "        f.write(f\"=================================\\n\")\n",
    "        f.write(f\"Modularity (Q): {results['modularity']:.4f}\\n\")\n",
    "        f.write(f\"Total Communities: {results['num_communities']}\\n\\n\")\n",
    "        f.write(f\"TOP 10 STACKS:\\n\")\n",
    "        for s in results['top_communities_summary']:\n",
    "            f.write(\n",
    "                f\"Rank {s['Rank']} (ID {s['ID']}): \"\n",
    "                f\"Size {s['Size']} ({s['Share']:.2f}%) - \"\n",
    "                f\"Hubs: {s['Core Hubs']}\\n\"\n",
    "            )\n",
    "    print(f\"Summary saved to {filepath}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- STARTING LPA ANALYSIS ---\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load data\n",
    "        G_real = load_graph(EDGES_FILE)\n",
    "        \n",
    "        # 2. Run LPA\n",
    "        partition_lpa, q_lpa = run_lpa_community_detection(G_real)\n",
    "        \n",
    "        # 3. Analyze structure\n",
    "        results = analyze_structure(G_real, partition_lpa, q_lpa)\n",
    "        \n",
    "        # 4. Export results\n",
    "        print(\"\\n--- EXPORTING RESULTS ---\")\n",
    "        \n",
    "        # A. Text summary\n",
    "        save_txt_summary(results, os.path.join(OUTPUT_ROOT_PATH, \"analysis_summary_lpa.txt\"))\n",
    "        \n",
    "        # B. Strategy 1: PageRank core graph\n",
    "        # Note: LPA results may be less regular than Louvain;\n",
    "        # top 200 nodes may concentrate in a few large communities\n",
    "        G_core, part_core = extract_core_subgraph(\n",
    "            G_real,\n",
    "            partition_lpa,\n",
    "            results['pagerank_scores'],\n",
    "            PAGERANK_SAMPLE_NODES\n",
    "        )\n",
    "        export_for_visualization(\n",
    "            G_core,\n",
    "            part_core,\n",
    "            OUTPUT_ROOT_PATH,\n",
    "            \"1_pagerank_core_lpa.gexf\"\n",
    "        )\n",
    "        \n",
    "        # C. Strategy 2: abstract community graph\n",
    "        G_abstract = build_community_graph(\n",
    "            G_real,\n",
    "            partition_lpa,\n",
    "            results['community_hubs']\n",
    "        )\n",
    "        # For the abstract graph, the partition is the community ID itself\n",
    "        abstract_part = {n: n for n in G_abstract.nodes()}\n",
    "        export_for_visualization(\n",
    "            G_abstract,\n",
    "            abstract_part,\n",
    "            OUTPUT_ROOT_PATH,\n",
    "            \"2_abstract_community_network_lpa.gexf\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n LPA Analysis COMPLETE!\")\n",
    "        print(f\"Files saved in: {OUTPUT_ROOT_PATH}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n FATAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f185fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING LPA ANALYSIS ON RANDOM DATA ---\n",
      "\n",
      "Processing random_graph_1.pkl...\n",
      "Loading random graph from: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_1.pkl\n",
      "  Graph loaded. Nodes=397797, Edges=1819936\n",
      "  Running LPA (Label Propagation)...\n",
      "  LPA Complete. Modularity Q: 0.0038\n",
      "  Exporting summary to E:\\Network Science Project\\02_LPA_method\\RandomData\\random_graph_1_output\\analysis_summary_random_lpa.txt...\n",
      "  Exporting partition CSV to E:\\Network Science Project\\02_LPA_method\\RandomData\\random_graph_1_output\\random_full_partition_random_lpa.csv...\n",
      "\n",
      "Processing random_graph_2.pkl...\n",
      "Loading random graph from: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_2.pkl\n",
      "  Graph loaded. Nodes=397797, Edges=1819936\n",
      "  Running LPA (Label Propagation)...\n",
      "  LPA Complete. Modularity Q: 0.0035\n",
      "  Exporting summary to E:\\Network Science Project\\02_LPA_method\\RandomData\\random_graph_2_output\\analysis_summary_random_lpa.txt...\n",
      "  Exporting partition CSV to E:\\Network Science Project\\02_LPA_method\\RandomData\\random_graph_2_output\\random_full_partition_random_lpa.csv...\n",
      "\n",
      "Processing random_graph_3.pkl...\n",
      "Loading random graph from: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_3.pkl\n",
      "  Graph loaded. Nodes=397797, Edges=1819936\n",
      "  Running LPA (Label Propagation)...\n",
      "  LPA Complete. Modularity Q: 0.0037\n",
      "  Exporting summary to E:\\Network Science Project\\02_LPA_method\\RandomData\\random_graph_3_output\\analysis_summary_random_lpa.txt...\n",
      "  Exporting partition CSV to E:\\Network Science Project\\02_LPA_method\\RandomData\\random_graph_3_output\\random_full_partition_random_lpa.csv...\n",
      "\n",
      "Processing random_graph_4.pkl...\n",
      "Loading random graph from: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_4.pkl\n",
      "  Graph loaded. Nodes=397797, Edges=1819936\n",
      "  Running LPA (Label Propagation)...\n",
      "  LPA Complete. Modularity Q: 0.0037\n",
      "  Exporting summary to E:\\Network Science Project\\02_LPA_method\\RandomData\\random_graph_4_output\\analysis_summary_random_lpa.txt...\n",
      "  Exporting partition CSV to E:\\Network Science Project\\02_LPA_method\\RandomData\\random_graph_4_output\\random_full_partition_random_lpa.csv...\n",
      "\n",
      "Processing random_graph_5.pkl...\n",
      "Loading random graph from: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_5.pkl\n",
      "  Graph loaded. Nodes=397797, Edges=1819936\n",
      "  Running LPA (Label Propagation)...\n",
      "  LPA Complete. Modularity Q: 0.0037\n",
      "  Exporting summary to E:\\Network Science Project\\02_LPA_method\\RandomData\\random_graph_5_output\\analysis_summary_random_lpa.txt...\n",
      "  Exporting partition CSV to E:\\Network Science Project\\02_LPA_method\\RandomData\\random_graph_5_output\\random_full_partition_random_lpa.csv...\n",
      "\n",
      " All Random Data LPA analyses complete.\n"
     ]
    }
   ],
   "source": [
    "# (Objective5) 02_LPA_RandomData\n",
    "import pickle\n",
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from community import community_louvain  # Used to compute modularity Q\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# INPUT: Random graphs generated for Louvain baseline\n",
    "BASE_INPUT_PATH = r\"E:\\Network Science Project\\01_Louvain_method\\RandomData\"\n",
    "\n",
    "# OUTPUT: LPA analysis results on random data\n",
    "BASE_OUTPUT_PATH = r\"E:\\Network Science Project\\02_LPA_method\\RandomData\"\n",
    "\n",
    "# Number of files to process\n",
    "NUM_GRAPHS = 5\n",
    "\n",
    "# ==========================================\n",
    "# 2. CORE FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def load_random_graph(file_path: str) -> nx.DiGraph:\n",
    "    \"\"\"Load a random graph in .pkl format\"\"\"\n",
    "    print(f\"Loading random graph from: {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "    \n",
    "    print(f\"  Graph loaded. Nodes={G.number_of_nodes()}, Edges={G.number_of_edges()}\")\n",
    "    return G\n",
    "\n",
    "def run_lpa_detection(G: nx.DiGraph):\n",
    "    \"\"\"Run the LPA algorithm\"\"\"\n",
    "    print(\"  Running LPA (Label Propagation)...\")\n",
    "    \n",
    "    # LPA requires an undirected graph\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    # Run LPA\n",
    "    lpa_gen = nx_comm.label_propagation_communities(G_undirected)\n",
    "    \n",
    "    # Convert to partition dictionary {node: community_id}\n",
    "    partition = {}\n",
    "    for idx, nodes in enumerate(lpa_gen):\n",
    "        for node in nodes:\n",
    "            partition[node] = idx\n",
    "            \n",
    "    # Compute modularity Q (using Louvain library standard)\n",
    "    try:\n",
    "        modularity = community_louvain.modularity(partition, G_undirected)\n",
    "    except Exception:\n",
    "        modularity = 0.0\n",
    "        \n",
    "    print(f\"  LPA Complete. Modularity Q: {modularity:.4f}\")\n",
    "    return partition, modularity\n",
    "\n",
    "def analyze_and_export(G, partition, modularity, output_dir, filename_suffix):\n",
    "    \"\"\"Analyze Top 10 and export summary and CSV\"\"\"\n",
    "    \n",
    "    # 1. Compute community sizes\n",
    "    comm_map = defaultdict(list)\n",
    "    for node, comm_id in partition.items():\n",
    "        comm_map[comm_id].append(node)\n",
    "        \n",
    "    sorted_comms = sorted(comm_map.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    total_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # 2. Generate summary text\n",
    "    summary_path = os.path.join(output_dir, f\"analysis_summary_{filename_suffix}.txt\")\n",
    "    \n",
    "    print(f\"  Exporting summary to {summary_path}...\")\n",
    "    \n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"NETWORK ANALYSIS SUMMARY (LPA on Random Data)\\n\")\n",
    "        f.write(f\"=============================================\\n\")\n",
    "        f.write(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Modularity Score (Q): {modularity:.4f}\\n\")\n",
    "        f.write(f\"Total Communities: {len(sorted_comms)}\\n\\n\")\n",
    "        f.write(f\"--- TOP 10 LARGEST STACKS (LPA) ---\\n\")\n",
    "        \n",
    "        for rank, (comm_id, nodes) in enumerate(sorted_comms[:10]):\n",
    "            size = len(nodes)\n",
    "            share = (size / total_nodes) * 100\n",
    "            sample_nodes = list(nodes)[:5]\n",
    "            f.write(\n",
    "                f\"Stack #{rank+1} (ID: {comm_id}): \"\n",
    "                f\"Size={size} ({share:.2f}%) -> Sample Nodes: {sample_nodes}\\n\"\n",
    "            )\n",
    "            \n",
    "    # 3. Export full partition CSV\n",
    "    csv_path = os.path.join(output_dir, f\"random_full_partition_{filename_suffix}.csv\")\n",
    "    print(f\"  Exporting partition CSV to {csv_path}...\")\n",
    "    \n",
    "    df = pd.DataFrame(list(partition.items()), columns=['Package', 'CommunityID'])\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- STARTING LPA ANALYSIS ON RANDOM DATA ---\")\n",
    "    \n",
    "    random_files = [f\"random_graph_{i}.pkl\" for i in range(1, NUM_GRAPHS + 1)]\n",
    "    \n",
    "    for filename in random_files:\n",
    "        print(f\"\\nProcessing {filename}...\")\n",
    "        \n",
    "        # Input path\n",
    "        input_path = os.path.join(BASE_INPUT_PATH, filename)\n",
    "        \n",
    "        # Output folder\n",
    "        folder_name = filename.replace('.pkl', '_output')\n",
    "        output_dir = os.path.join(BASE_OUTPUT_PATH, folder_name)\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # 1. Load graph\n",
    "            G_rand = load_random_graph(input_path)\n",
    "            \n",
    "            # 2. Run LPA\n",
    "            partition_lpa, q_lpa = run_lpa_detection(G_rand)\n",
    "            \n",
    "            # 3. Export results\n",
    "            analyze_and_export(\n",
    "                G_rand,\n",
    "                partition_lpa,\n",
    "                q_lpa,\n",
    "                output_dir,\n",
    "                \"random_lpa\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {filename}: {e}\")\n",
    "\n",
    "    print(\"\\n All Random Data LPA analyses complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
