{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from community import community_louvain \n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION (已修正输出路径)\n",
    "# ==========================================\n",
    "\n",
    "# 原始数据输入路径\n",
    "# 根据你的截图，数据文件位于此路径\n",
    "DATA_PATH = r\"E:\\Network Science Project\\pypi_dag\"\n",
    "EDGES_FILE = os.path.join(DATA_PATH, \"edges.csv\") \n",
    "\n",
    "# 输出路径：将在 RealData 文件夹下新建一个 LPA_Analysis 文件夹\n",
    "OUTPUT_ROOT_PATH = r\"E:\\Network Science Project\\RealData\\LPA_Analysis\"\n",
    "os.makedirs(OUTPUT_ROOT_PATH, exist_ok=True) \n",
    "\n",
    "# GEXF 可视化参数\n",
    "PAGERANK_SAMPLE_NODES = 200 # 用于文件 1 的 PageRank 采样节点数\n",
    "\n",
    "# ==========================================\n",
    "# 2. CORE FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def load_graph(edges_file: str) -> nx.DiGraph:\n",
    "    \"\"\"从 edges.csv 文件加载有向图.\"\"\"\n",
    "    print(f\"Loading graph from {edges_file}...\")\n",
    "    # 尝试从 CSV 加载边数据\n",
    "    try:\n",
    "        df_edges = pd.read_csv(edges_file)\n",
    "        # 假设边文件包含 'source' 和 'target' 两列\n",
    "        G = nx.from_pandas_edgelist(\n",
    "            df_edges, \n",
    "            source='source', \n",
    "            target='target', \n",
    "            create_graph=nx.DiGraph()\n",
    "        )\n",
    "        print(f\"Graph loaded. Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
    "        return G\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading graph: {e}\")\n",
    "        raise FileNotFoundError(f\"Could not load graph from {edges_file}\")\n",
    "\n",
    "def run_lpa_community_detection(G: nx.DiGraph) -> dict:\n",
    "    \"\"\"运行 Label Propagation Algorithm (LPA)\"\"\"\n",
    "    print(\"Running Label Propagation Algorithm (LPA)...\")\n",
    "    \n",
    "    # networkx 的 LPA 要求无向图\n",
    "    lpa_partitions = nx.label_propagation_communities(G.to_undirected()) \n",
    "    \n",
    "    partition = {}\n",
    "    for i, nodes in enumerate(lpa_partitions):\n",
    "        for node in nodes:\n",
    "            partition[node] = i\n",
    "    \n",
    "    # 使用 Louvain 的模块度函数来评估 LPA 的结果\n",
    "    modularity_score = community_louvain.modularity(partition, G)\n",
    "    print(f\"LPA completed. Modularity Score (Q): {modularity_score:.4f}\")\n",
    "    return partition, modularity_score\n",
    "\n",
    "def analyze_community_structure(G: nx.DiGraph, partition: dict, modularity_score: float) -> dict:\n",
    "    \"\"\"分析 LPA 结果，提取 Top 10 Stacks 和摘要.\"\"\"\n",
    "    \n",
    "    community_map = defaultdict(list)\n",
    "    for node, comm_id in partition.items():\n",
    "        community_map[comm_id].append(node)\n",
    "    \n",
    "    sorted_communities = sorted(community_map.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    num_communities = len(sorted_communities)\n",
    "    total_nodes = G.number_of_nodes()\n",
    "    top_10_summary = []\n",
    "    \n",
    "    # 预计算 PageRank 用于 Hubs 识别\n",
    "    print(\"Calculating PageRank for Core Hubs...\")\n",
    "    pagerank_scores = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "    print(\"\\n--- TOP 10 LARGEST STACKS ---\")\n",
    "    for rank, (comm_id, nodes) in enumerate(sorted_communities[:10]):\n",
    "        size_nodes = len(nodes)\n",
    "        share_percent = (size_nodes / total_nodes) * 100\n",
    "        \n",
    "        # 提取 Core Hubs (基于 PageRank)\n",
    "        node_pr_tuples = [(node, pagerank_scores.get(node, 0)) for node in nodes]\n",
    "        top_hubs = sorted(node_pr_tuples, key=lambda x: x[1], reverse=True)[:8]\n",
    "        top_hubs_names = [name for name, pr in top_hubs]\n",
    "\n",
    "        stack_info = {\n",
    "            'Rank': rank + 1,\n",
    "            'ID': comm_id,\n",
    "            'Size': size_nodes,\n",
    "            'Share': share_percent,\n",
    "            'Core Hubs': ', '.join(top_hubs_names)\n",
    "        }\n",
    "        top_10_summary.append(stack_info)\n",
    "        \n",
    "        print(f\"Stack #{rank+1} (ID: {comm_id}): Size={size_nodes} ({share_percent:.2f}%)\")\n",
    "        print(f\"  Core Hubs: {stack_info['Core Hubs']}\")\n",
    "        \n",
    "    results = {\n",
    "        \"graph_name\": \"Real PyPI Network (LPA)\",\n",
    "        \"modularity\": modularity_score,\n",
    "        \"num_communities\": num_communities,\n",
    "        \"partition\": partition,\n",
    "        \"top_communities_summary\": top_10_summary, \n",
    "        \"total_nodes\": total_nodes,\n",
    "        \"pagerank_scores\": pagerank_scores\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def save_analysis_summary(results: dict, G: nx.DiGraph, output_path: str):\n",
    "    \"\"\"将分析结果保存到文本文件.\"\"\"\n",
    "    \n",
    "    total_share = sum(d['Share'] for d in results['top_communities_summary'])\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "==================================================\n",
    "        NETWORK ANALYSIS SUMMARY (LPA)\n",
    "==================================================\n",
    "Date: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "Graph Name: {results['graph_name']}\n",
    "\n",
    "--- GRAPH DIAGNOSTICS ---\n",
    "Nodes (Packages): {results['total_nodes']}\n",
    "Edges (Dependencies): {G.number_of_edges()}\n",
    "DAG Property: Valid (No cycles) \n",
    "\n",
    "--- COMMUNITY STRUCTURE (LPA) ---\n",
    "Modularity Score (Q): {results['modularity']:.4f} \n",
    "Total Communities Found: {results['num_communities']}\n",
    "\n",
    "--- TOP 10 LARGEST STACKS ---\n",
    "(Total Share: {total_share:.2f}%)\n",
    "\"\"\"\n",
    "    \n",
    "    for stack in results['top_communities_summary']:\n",
    "        summary_text += f\"\"\"\n",
    "Stack #{stack['Rank']} (ID: {stack['ID']}): Size={stack['Size']} ({stack['Share']:.2f}%)\n",
    "  Core Hubs: {stack['Core Hubs']}\n",
    "\"\"\"\n",
    "    \n",
    "    summary_text += \"\\n==================================================\"\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_text)\n",
    "    \n",
    "    print(f\"\\nAnalysis summary saved to: {output_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. GEXF EXPORT FUNCTIONS (PLACEHOLDERS)\n",
    "# ==========================================\n",
    "\n",
    "# ⚠️ WARNING: 你必须在此处粘贴你原始脚本中用于生成 GEXF 文件的函数！\n",
    "# 这些函数通常包括：\n",
    "# 1. def build_pagerank_core(...)  # 用于生成 1_...gexf\n",
    "# 2. def build_community_graph(...) # 用于生成 3_...gexf\n",
    "# 如果你没有这些函数，GEXF 文件将不会生成。\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. 加载图\n",
    "    try:\n",
    "        G_real = load_graph(EDGES_FILE)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"FATAL ERROR: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    # 2. 运行 LPA 社区检测\n",
    "    partition_lpa, modularity_lpa = run_lpa_community_detection(G_real)\n",
    "    \n",
    "    # 3. 分析结果\n",
    "    analysis_results = analyze_community_structure(G_real, partition_lpa, modularity_lpa)\n",
    "    \n",
    "    # 4. 保存摘要\n",
    "    summary_output_path = os.path.join(OUTPUT_ROOT_PATH, \"analysis_summary_lpa.txt\")\n",
    "    save_analysis_summary(analysis_results, G_real, summary_output_path)\n",
    "    \n",
    "    # 5. ⚠️ 调用 GEXF 导出函数 (需要你补充)\n",
    "    # print(\"\\nStarting GEXF export...\")\n",
    "    # if 'build_pagerank_core' in locals():\n",
    "    #     build_pagerank_core(G_real, analysis_results['pagerank_scores'], analysis_results['partition'], OUTPUT_ROOT_PATH, PAGERANK_SAMPLE_NODES)\n",
    "    # if 'build_community_graph' in locals():\n",
    "    #     build_community_graph(G_real, analysis_results['partition'], OUTPUT_ROOT_PATH)\n",
    "    \n",
    "    print(f\"\\nLPA analysis complete. Results saved to: {OUTPUT_ROOT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
