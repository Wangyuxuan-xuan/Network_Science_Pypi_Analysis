{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cb45ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Advanced Analysis and Automated Export ---\n",
      "\n",
      "Loading data from: E:\\Network Science Project\\pypi_dag\\edges.csv\n",
      "\n",
      "==================== Analyzing: REAL PyPI Dependency Network ====================\n",
      "[Step 1] Graph Diagnostics:\n",
      "  - Nodes: 397798\n",
      "  - Edges: 1819937\n",
      "  - DAG Property: Valid (No cycles found)\n",
      "[Step 2] Creating Undirected Projection...\n",
      "[Step 3] Running Louvain Algorithm (optimizing Modularity)...\n",
      " Optimization Complete.\n",
      " Modularity Score (Q): 0.4755\n",
      "[Step 4] Extracting Technology Stacks...\n",
      "  - Total Communities Found: 1540\n",
      "  - Details of Top 10 Largest Communities (Potential Tech Stacks):\n",
      " [Stack #1] ID: 6 | Size: 80618 (20.27%)\n",
      "  -> Core Packages (Hubs): numpy, pandas, matplotlib, scipy, scikit-learn, seaborn, opencv-python, networkx\n",
      " [Stack #2] ID: 4 | Size: 57375 (14.42%)\n",
      "  -> Core Packages (Hubs): pydantic, tqdm, rich, torch, python-dotenv, httpx, typer, fastapi\n",
      " [Stack #3] ID: 194 | Size: 47400 (11.92%)\n",
      "  -> Core Packages (Hubs): pyyaml, click, jinja2, psutil, boto3, colorama, tabulate, toml\n",
      " [Stack #4] ID: 1 | Size: 45894 (11.54%)\n",
      "  -> Core Packages (Hubs): typing-extensions, python-dateutil, urllib3, six, flask, packaging, cryptography, pytz\n",
      " [Stack #5] ID: 238 | Size: 43074 (10.83%)\n",
      "  -> Core Packages (Hubs): requests, beautifulsoup4, lxml, selenium, bs4, appdirs, argparse, xmltodict\n",
      " [Stack #6] ID: 38 | Size: 18993 (4.77%)\n",
      "  -> Core Packages (Hubs): odoo, python-stdnum, odoo14-addon-ssi-master-data-mixin, odoo14-addon-ssi-transaction-cancel-mixin, odoo14-addon-ssi-transaction-confirm-mixin, odoo14-addon-ssi-transaction-done-mixin, openupgradelib, odoo-addon-queue-job\n",
      " [Stack #7] ID: 101 | Size: 15684 (3.94%)\n",
      "  -> Core Packages (Hubs): django, redis, pymongo, djangorestframework, celery, sentry-sdk, elasticsearch, pika\n",
      " [Stack #8] ID: 9 | Size: 15221 (3.83%)\n",
      "  -> Core Packages (Hubs): sqlalchemy, openpyxl, pyarrow, pycryptodome, pyqt5, psycopg2-binary, polars, pymysql\n",
      " [Stack #9] ID: 5 | Size: 13229 (3.33%)\n",
      "  -> Core Packages (Hubs): pillow, pygame, pywin32, nonebot2, pyside6, reportlab, pyautogui, qrcode\n",
      " [Stack #10] ID: 10 | Size: 10982 (2.76%)\n",
      "  -> Core Packages (Hubs): pytest, sphinx, black, flake8, pytest-cov, coverage, pre-commit, mypy\n",
      "\n",
      "[Data Export] Generating Analysis Summary to E:\\Network Science Project\\01_Louvain_method\\RealData\\analysis_summary.txt...\n",
      " Analysis Summary TXT saved.\n",
      "\n",
      "[Data Export] Saving full partition to E:\\Network Science Project\\01_Louvain_method\\RealData\\pypi_full_partition_realdata.csv...\n",
      " Full partition CSV saved.\n",
      "\n",
      "[Data Export] Generating Top 5 Stacks summary table to E:\\Network Science Project\\01_Louvain_method\\RealData\\top_5_stacks_summary.csv...\n",
      " Top Stacks Summary CSV saved.\n",
      "\n",
      "[--- Starting Visualization Exports ---]\n",
      "\n",
      "[Visualization Prep] Calculating PageRank and extracting top 200 core nodes...\n",
      "  - Core Subgraph Extracted (PageRank): Nodes=200, Edges=329\n",
      " Exported: 1_pagerank_core_for_gephi.gexf (Nodes: 200, Edges: 329)\n",
      "\n",
      "[Visualization Prep] Extracting Top 5 largest communities...\n",
      "  - Top Communities Subgraph Extracted: Nodes=274361, Edges=1291203\n",
      " Exported: 2_top_5_communities_for_gephi.gexf (Nodes: 274361, Edges: 1291203)\n",
      "\n",
      "[Visualization Prep] Building abstract Community Graph (Strategy 2)...\n",
      "  - Community Graph built. Nodes=1540, Edges=1301\n",
      " Exported: 3_abstract_community_network.gexf (Nodes: 1540, Edges: 1301)\n",
      "[--- Visualization Exports Complete ---]\n",
      "\n",
      "==================================================\n",
      " ALL EXPORTS COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# (Objective3) 01_Louvain_RealData\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import community.community_louvain as community_louvain\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION SECTION (real data)\n",
    "# ==========================================\n",
    "\n",
    "DATA_PATH = r\"E:\\Network Science Project\\pypi_dag\" \n",
    "EDGES_FILE_NAME = \"edges.csv\"\n",
    "OUTPUT_ROOT_PATH = r\"E:\\Network Science Project\\01_Louvain_method\\RealData\"\n",
    "\n",
    "# --- Visualization Sampler Settings ---\n",
    "# PageRank Sample Size: Extracts the N most influential nodes (for showing Macro Authority Structure)\n",
    "PAGERANK_SAMPLE_NODES = 200 \n",
    "# Core Communities Sample Size: Extracts all nodes belonging to the TOP K largest communities \n",
    "TOP_COMMUNITIES_SAMPLE = 5 \n",
    "# ------------------------------------\n",
    "\n",
    "# ==========================================\n",
    "# 2. UTILITIES\n",
    "# ==========================================\n",
    "\n",
    "def load_data(data_dir: str, filename: str) -> nx.DiGraph:\n",
    "    \"\"\"Loads data, handles pathing, and builds the directed graph.\"\"\"\n",
    "    file_path = os.path.join(data_dir, filename) \n",
    "    print(f\"\\nLoading data from: {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        if not filename.endswith('.csv') and os.path.exists(os.path.join(data_dir, filename + '.csv')):\n",
    "             file_path = os.path.join(data_dir, filename + '.csv')\n",
    "        else:\n",
    "             raise FileNotFoundError(f\" Error: Could not find file at {file_path}. Check DATA_PATH and EDGES_FILE_NAME.\")\n",
    "    \n",
    "    edges_df = pd.read_csv(file_path, usecols=['source', 'target'])\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        edges_df, source='source', target='target', create_using=nx.DiGraph()\n",
    "    )\n",
    "    return G\n",
    "\n",
    "def export_partition_to_csv(partition: dict, output_path: str, filename: str = \"pypi_full_partition_realdata.csv\"):\n",
    "    \"\"\"\n",
    "    Exports the complete community partition to the specified CSV file.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(output_path, filename)\n",
    "    print(f\"\\n[Data Export] Saving full partition to {file_path}...\")\n",
    "    \n",
    "    partition_df = pd.DataFrame(\n",
    "        list(partition.items()), \n",
    "        columns=['PackageName', 'CommunityID']\n",
    "    )\n",
    "    \n",
    "    partition_df.to_csv(file_path, index=False)\n",
    "    print(f\" Full partition CSV saved.\")\n",
    "\n",
    "def export_top_stacks_table(G: nx.DiGraph, sorted_communities: list, top_k: int, output_path: str, filename: str = \"top_5_stacks_summary.csv\"):\n",
    "    \"\"\"\n",
    "    Exports the core data for the Top K Stacks as a CSV table for reporting (Method 2 Table).\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(output_path, filename)\n",
    "    print(f\"\\n[Data Export] Generating Top {top_k} Stacks summary table to {file_path}...\")\n",
    "    \n",
    "    table_data = []\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    \n",
    "    for i, (comm_id, nodes) in enumerate(sorted_communities[:top_k]):\n",
    "        top_hubs = sorted(nodes, key=lambda x: G.in_degree(x), reverse=True)[:8]\n",
    "        share = (len(nodes) / num_nodes) * 100\n",
    "        \n",
    "        table_data.append({\n",
    "            'Stack #': i + 1,\n",
    "            'Community ID': comm_id,\n",
    "            'Size (Nodes)': len(nodes),\n",
    "            'Share (%)': f'{share:.2f}',\n",
    "            'Core Packages (Hubs)': ', '.join(top_hubs)\n",
    "        })\n",
    "        \n",
    "    df_summary = pd.DataFrame(table_data)\n",
    "    df_summary.to_csv(file_path, index=False)\n",
    "    print(f\" Top Stacks Summary CSV saved.\")\n",
    "\n",
    "# ==========================================\n",
    "# FUNCTION 1: CORE ANALYSIS LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def analyze_community_structure(G, graph_name=\"Unknown Graph\", top_k_stacks=5):\n",
    "    \"\"\"\n",
    "    Function 1: Objective 3 Core Logic\n",
    "    \n",
    "    This function performs community detection on a given directed graph (G).\n",
    "    It follows the standard pipeline: \n",
    "    Check Stats -> Undirected Projection -> Louvain Algorithm -> Result Parsing.\n",
    "\n",
    "    Args:\n",
    "        G (nx.DiGraph): The input directed graph (Real or Random).\n",
    "        graph_name (str): A label for printing (e.g., \"Real PyPI\", \"Random #1\").\n",
    "        top_k_stacks (int): How many top communities to display in detail.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results:\n",
    "              - 'modularity': The Q score (float).\n",
    "              - 'num_communities': Total communities found (int).\n",
    "              - 'partition': The raw partition dict {node: community_id}.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*20} Analyzing: {graph_name} {'='*20}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Step 1: Basic Graph Diagnostics\n",
    "    # ---------------------------------------------------------\n",
    "    # Basic statistics\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    print(f\"[Step 1] Graph Diagnostics:\")\n",
    "    print(f\"  - Nodes: {num_nodes}\")\n",
    "    print(f\"  - Edges: {num_edges}\")\n",
    "\n",
    "    # Check for DAG property (Cycle detection)\n",
    "    # Note: While Louvain works on undirected graphs, verifying the DAG property\n",
    "    # is crucial for the integrity of the dependency network dataset.\n",
    "    if nx.is_directed_acyclic_graph(G):\n",
    "        print(f\"  - DAG Property: Valid (No cycles found)\")\n",
    "    else:\n",
    "        # If it's a random graph, strict DAG compliance depends on the generation method.\n",
    "        # We print a warning but proceed, as Louvain doesn't require acyclicity.\n",
    "        print(f\"  - DAG Property: Cycles detected\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Step 2: Undirected Projection\n",
    "    # ---------------------------------------------------------\n",
    "    # Louvain algorithm maximizes modularity on Undirected Graphs.\n",
    "    # We project the directed dependencies (A->B) to a mutual connection (A-B).\n",
    "    # This captures the \"semantic relationship\" between packages.\n",
    "    print(f\"[Step 2] Creating Undirected Projection...\")\n",
    "    G_undirected = G.to_undirected()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Step 3: Louvain Optimization\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"[Step 3] Running Louvain Algorithm (optimizing Modularity)...\")\n",
    "    \n",
    "    # partition is a dict: {node_name: community_id}\n",
    "    # This function iteratively optimizes the Modularity score (Q).\n",
    "    try:\n",
    "        partition = community_louvain.best_partition(G_undirected)\n",
    "    except ValueError as e:\n",
    "        print(f\" Error during Louvain execution: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Calculate the final Modularity Score (Q)\n",
    "    # Range: [-0.5, 1.0]. Higher Q means stronger community structure.\n",
    "    modularity_score = community_louvain.modularity(partition, G_undirected)\n",
    "    print(f\" Optimization Complete.\")\n",
    "    print(f\" Modularity Score (Q): {modularity_score:.4f}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Step 4: Community / Stack Analysis\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"[Step 4] Extracting Technology Stacks...\")\n",
    "\n",
    "    # Group nodes by their community ID\n",
    "    community_map = defaultdict(list)\n",
    "    for node, comm_id in partition.items():\n",
    "        community_map[comm_id].append(node)\n",
    "    \n",
    "    # Sort communities by size (largest first)\n",
    "    sorted_communities = sorted(community_map.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    num_communities = len(sorted_communities)\n",
    "    print(f\"  - Total Communities Found: {num_communities}\")\n",
    "\n",
    "    print(f\"  - Details of Top {top_k_stacks} Largest Communities (Potential Tech Stacks):\")\n",
    "    \n",
    "    for i, (comm_id, nodes) in enumerate(sorted_communities[:top_k_stacks]):\n",
    "        # To name a stack, we look for \"Hub\" nodes (High In-Degree in the ORIGINAL DiGraph).\n",
    "        # High In-Degree = Highly depended upon = Core package of that stack.\n",
    "        # We define 'importance' by In-Degree here.\n",
    "        top_hubs = sorted(nodes, key=lambda x: G.in_degree(x), reverse=True)[:8]\n",
    "        \n",
    "        # Calculate percentage of total graph\n",
    "        share = (len(nodes) / num_nodes) * 100\n",
    "        \n",
    "        print(f\" [Stack #{i+1}] ID: {comm_id} | Size: {len(nodes)} ({share:.2f}%)\")\n",
    "        print(f\"  -> Core Packages (Hubs): {', '.join(top_hubs)}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Return Results Package\n",
    "    # ---------------------------------------------------------\n",
    "    results = {\n",
    "        \"graph_name\": graph_name,\n",
    "        \"modularity\": modularity_score,\n",
    "        \"num_communities\": num_communities,\n",
    "        \"partition\": partition,\n",
    "        \"top_communities_summary\": sorted_communities[:top_k_stacks]\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. VISUALIZATION PREPARATION FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def extract_core_subgraph(G: nx.DiGraph, partition: dict, num_nodes: int) -> tuple[nx.DiGraph, dict]:\n",
    "    \"\"\"\n",
    "    Method 1: Extracts the core subgraph based on PageRank authority (Top N nodes).\n",
    "    Used to visualize the macro-structure and distribution of authoritative packages.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Visualization Prep] Calculating PageRank and extracting top {num_nodes} core nodes...\")\n",
    "    \n",
    "    # Calculate PageRank (measures a node's influence/authority)\n",
    "    pagerank_scores = nx.pagerank(G, alpha=0.85)\n",
    "    top_nodes = sorted(pagerank_scores, key=pagerank_scores.get, reverse=True)[:num_nodes]\n",
    "    \n",
    "    # Induced subgraph: only includes selected nodes and the edges between them\n",
    "    core_subgraph = G.subgraph(top_nodes).copy()\n",
    "    \n",
    "    # Create the partition dictionary for the subgraph\n",
    "    core_partition = {node: partition[node] for node in core_subgraph.nodes() if node in partition}\n",
    "    \n",
    "    print(f\"  - Core Subgraph Extracted (PageRank): Nodes={core_subgraph.number_of_nodes()}, Edges={core_subgraph.number_of_edges()}\")\n",
    "    return core_subgraph, core_partition\n",
    "\n",
    "\n",
    "def extract_top_community_subgraph(G: nx.DiGraph, sorted_communities: list, top_k: int) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Method 2: Extracts the subgraph containing only the top K largest communities.\n",
    "    NOTE: This function should only return the graph object.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Visualization Prep] Extracting Top {top_k} largest communities...\")\n",
    "    \n",
    "    top_k_nodes = set()\n",
    "    for _, nodes in sorted_communities[:top_k]:\n",
    "        top_k_nodes.update(nodes)\n",
    "        \n",
    "    top_comm_subgraph = G.subgraph(top_k_nodes).copy()\n",
    "    \n",
    "    print(f\"  - Top Communities Subgraph Extracted: Nodes={top_comm_subgraph.number_of_nodes()}, Edges={top_comm_subgraph.number_of_edges()}\")\n",
    "    return top_comm_subgraph\n",
    "\n",
    "\n",
    "def build_community_graph(G: nx.DiGraph, partition: dict) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Model2.2: construct abstract community network (Community Graph)ã€‚\n",
    "    each node: one community; each edge: the number of dependencies between communities\n",
    "    \"\"\"\n",
    "    print(\"\\n[Visualization Prep] Building abstract Community Graph (Strategy 2)...\")\n",
    "    \n",
    "    G_comm = nx.DiGraph()\n",
    "    community_sizes = defaultdict(int)\n",
    "    inter_community_edges = defaultdict(int)\n",
    "    \n",
    "    # 1. calculate community size and edge weight between communities\n",
    "    for node, comm_id in partition.items():\n",
    "        community_sizes[comm_id] += 1\n",
    "        \n",
    "    for u, v in G.edges():\n",
    "        if u in partition and v in partition:\n",
    "            comm_u = partition[u]\n",
    "            comm_v = partition[v]\n",
    "            \n",
    "            # dependencies in diff comm\n",
    "            if comm_u != comm_v:\n",
    "                inter_community_edges[(comm_u, comm_v)] += 1\n",
    "                \n",
    "    # 2. construct G_comm\n",
    "    \n",
    "    # add notes (CommunityNode)\n",
    "    for comm_id, size in community_sizes.items():\n",
    "        G_comm.add_node(\n",
    "            comm_id, \n",
    "            size=size,\n",
    "            label=f'Community {comm_id}',\n",
    "            type='CommunityNode'\n",
    "        )\n",
    "        \n",
    "    # add edges (InterCommunityDependency)\n",
    "    for (comm_u, comm_v), weight in inter_community_edges.items():\n",
    "        G_comm.add_edge(\n",
    "            comm_u, \n",
    "            comm_v, \n",
    "            weight=weight,\n",
    "            type='InterCommunityDependency'\n",
    "        )\n",
    "        \n",
    "    print(f\"  - Community Graph built. Nodes={G_comm.number_of_nodes()}, Edges={G_comm.number_of_edges()}\")\n",
    "    return G_comm\n",
    "\n",
    "def export_for_visualization(G: nx.DiGraph, partition: dict, output_path: str, filename: str):\n",
    "    \"\"\"\n",
    "    Prepares the graph by adding necessary node/edge attributes and exports it to Gephi (.gexf format).\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(output_path, filename)\n",
    "    \n",
    "    G_export = G.copy()\n",
    "    \n",
    "    # set node attributes: CommunityID\n",
    "    nx.set_node_attributes(G_export, partition, name='CommunityID')\n",
    "    \n",
    "    # set edge attributes: LinkType (Internal/Inter-Community)\n",
    "    edge_type = {}\n",
    "    for u, v in G_export.edges():\n",
    "        u_comm = partition.get(u, -1)\n",
    "        v_comm = partition.get(v, -1)\n",
    "        \n",
    "        if u_comm == v_comm and u_comm != -1:\n",
    "            edge_type[(u, v)] = 'Internal'\n",
    "        else:\n",
    "            edge_type[(u, v)] = 'Inter-Community'\n",
    "            \n",
    "    nx.set_edge_attributes(G_export, edge_type, name='LinkType')\n",
    "    \n",
    "    # export to GEXF\n",
    "    nx.write_gexf(G_export, file_path)\n",
    "    print(f\" Exported: {filename} (Nodes: {G_export.number_of_nodes()}, Edges: {G_export.number_of_edges()})\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN EXECUTION (HOW TO RUN)\n",
    "# ==========================================\n",
    "\n",
    "def export_analysis_summary(G: nx.DiGraph, results: dict, output_path: str, filename: str = \"analysis_summary.txt\"):\n",
    "    \"\"\"\n",
    "    Exports all key summary data (Q, diagnostics, Top 10 Hubs) to a single TXT file.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(output_path, filename)\n",
    "    print(f\"\\n[Data Export] Generating Analysis Summary to {file_path}...\")\n",
    "\n",
    "    # Top 10 Stacks' Hubs info\n",
    "    stacks_text = \"\\n\"\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    \n",
    "    for i, (comm_id, nodes) in enumerate(results['top_communities_summary'][:10]):\n",
    "        top_hubs = sorted(nodes, key=lambda x: G.in_degree(x), reverse=True)[:8]\n",
    "        share = (len(nodes) / num_nodes) * 100\n",
    "        \n",
    "        stacks_text += f\"Stack #{i+1} (ID: {comm_id}): Size={len(nodes)} ({share:.2f}%)\\n\"\n",
    "        stacks_text += f\"  Core Hubs: {', '.join(top_hubs)}\\n\"\n",
    "    \n",
    "    summary_content = f\"\"\"\n",
    "==================================================\n",
    "        NETWORK ANALYSIS SUMMARY\n",
    "==================================================\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Graph Name: {results['graph_name']}\n",
    "\n",
    "--- GRAPH DIAGNOSTICS ---\n",
    "Nodes (Packages): {G.number_of_nodes()}\n",
    "Edges (Dependencies): {G.number_of_edges()}\n",
    "DAG Property: {'Valid (No cycles)' if nx.is_directed_acyclic_graph(G) else 'Cycles Detected'}\n",
    "\n",
    "--- COMMUNITY STRUCTURE (Louvain) ---\n",
    "Modularity Score (Q): {results['modularity']:.4f}  <-- Objective 5 BASELINE\n",
    "Total Communities Found: {results['num_communities']}\n",
    "\n",
    "--- TOP 10 LARGEST STACKS ---\n",
    "{stacks_text}\n",
    "==================================================\n",
    "\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_content)\n",
    "    print(f\" Analysis Summary TXT saved.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    os.makedirs(OUTPUT_ROOT_PATH, exist_ok=True)\n",
    "    print(\"--- Starting Advanced Analysis and Automated Export ---\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load Data\n",
    "        G_real = load_data(DATA_PATH, EDGES_FILE_NAME)\n",
    "\n",
    "        # 2. Call Function 1: Analyze Community Structure (Objective 3)\n",
    "        real_results = analyze_community_structure(\n",
    "            G_real, \n",
    "            graph_name=\"REAL PyPI Dependency Network\",\n",
    "            top_k_stacks=10 # Console prints Top 10\n",
    "        )\n",
    "        partition = real_results['partition']\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        # 3. Export: Summary & Tables\n",
    "        # -----------------------------------------------------------------\n",
    "        \n",
    "        # 3.1 Export Summary.txt (Q value)\n",
    "        export_analysis_summary(G_real, real_results, OUTPUT_ROOT_PATH)\n",
    "        \n",
    "        # 3.2 Export full partition CSV\n",
    "        export_partition_to_csv(partition, OUTPUT_ROOT_PATH)\n",
    "        \n",
    "        # 3.3 partition Top 5 Stacks CSV \n",
    "        export_top_stacks_table(G_real, real_results['top_communities_summary'], TOP_COMMUNITIES_SAMPLE, OUTPUT_ROOT_PATH, filename=\"top_5_stacks_summary.csv\")\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # 4. VISUALIZATION PREPARATION (GEXF export)\n",
    "        # -----------------------------------------------------------------\n",
    "        print(\"\\n[--- Starting Visualization Exports ---]\")\n",
    "\n",
    "        # 4.1 Sample A: PageRank core subgraph (Model 1 - core nodes)\n",
    "        core_G, core_partition = extract_core_subgraph(\n",
    "            G_real, \n",
    "            partition, \n",
    "            num_nodes=PAGERANK_SAMPLE_NODES\n",
    "        )\n",
    "        export_for_visualization(\n",
    "            core_G, \n",
    "            core_partition, \n",
    "            OUTPUT_ROOT_PATH, \n",
    "            filename=\"1_pagerank_core_for_gephi.gexf\"\n",
    "        )\n",
    "\n",
    "        # 4.2 Sample B: Top 5 communities subgraph (whole communities nodes)\n",
    "        top_comm_G = extract_top_community_subgraph(\n",
    "            G_real, \n",
    "            real_results['top_communities_summary'], \n",
    "            top_k=TOP_COMMUNITIES_SAMPLE\n",
    "        )\n",
    "        top_comm_partition = {node: partition[node] for node in top_comm_G.nodes() if node in partition}\n",
    "\n",
    "        export_for_visualization(\n",
    "            top_comm_G, \n",
    "            top_comm_partition, \n",
    "            OUTPUT_ROOT_PATH, \n",
    "            filename=\"2_top_5_communities_for_gephi.gexf\" \n",
    "        )\n",
    "        \n",
    "        # 4.3 Sample C: abstract community graph (Model2 macro structure)\n",
    "        G_community = build_community_graph(G_real, partition)\n",
    "        \n",
    "        # Export G_community (use self index partition)\n",
    "        community_partition = {node: node for node in G_community.nodes()}\n",
    "        \n",
    "        export_for_visualization(\n",
    "            G_community, \n",
    "            community_partition, \n",
    "            OUTPUT_ROOT_PATH, \n",
    "            filename=\"3_abstract_community_network.gexf\"\n",
    "        )\n",
    "        print(\"[--- Visualization Exports Complete ---]\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- Analysis Failed ---\")\n",
    "        print(f\"Error encountered: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" ALL EXPORTS COMPLETE\")\n",
    "    print(\"==================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac40eb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Random Graph Baseline Analysis (Objective 5) ---\n",
      "\n",
      "Loading random graph from: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_1.pkl\n",
      "Graph loaded. Nodes=397797, Edges=1819936\n",
      "\n",
      "==================== Analyzing: Random Graph: random_graph_1.pkl ====================\n",
      "  Optimization Complete. Q Score: 0.2389\n",
      "  - Total Communities Found: 551\n",
      "-> Q Score recorded: 0.2389\n",
      "  - Generating Analysis Summary to E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_1_output\\analysis_summary_random.txt...\n",
      "  Analysis Summary TXT saved.\n",
      "  Full partition CSV saved to: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_1_output\\random_full_partition.csv\n",
      "  - Generating Top 5 Stacks summary table...\n",
      "  Top Stacks Summary CSV saved.\n",
      "  - [Vis Prep] Calculating PageRank (Top 100)...\n",
      "   Exported GEXF: 1_pagerank_core_for_gephi_random.gexf\n",
      "  - [Vis Prep] Extracting Top 5 largest communities...\n",
      "   Exported GEXF: 2_top_5_communities_for_gephi_random.gexf\n",
      "\n",
      "Loading random graph from: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_2.pkl\n",
      "Graph loaded. Nodes=397797, Edges=1819936\n",
      "\n",
      "==================== Analyzing: Random Graph: random_graph_2.pkl ====================\n",
      "  Optimization Complete. Q Score: 0.2386\n",
      "  - Total Communities Found: 515\n",
      "-> Q Score recorded: 0.2386\n",
      "  - Generating Analysis Summary to E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_2_output\\analysis_summary_random.txt...\n",
      "  Analysis Summary TXT saved.\n",
      "  Full partition CSV saved to: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_2_output\\random_full_partition.csv\n",
      "  - Generating Top 5 Stacks summary table...\n",
      "  Top Stacks Summary CSV saved.\n",
      "  - [Vis Prep] Calculating PageRank (Top 100)...\n",
      "   Exported GEXF: 1_pagerank_core_for_gephi_random.gexf\n",
      "  - [Vis Prep] Extracting Top 5 largest communities...\n",
      "   Exported GEXF: 2_top_5_communities_for_gephi_random.gexf\n",
      "\n",
      "Loading random graph from: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_3.pkl\n",
      "Graph loaded. Nodes=397797, Edges=1819936\n",
      "\n",
      "==================== Analyzing: Random Graph: random_graph_3.pkl ====================\n",
      "  Optimization Complete. Q Score: 0.2389\n",
      "  - Total Communities Found: 519\n",
      "-> Q Score recorded: 0.2389\n",
      "  - Generating Analysis Summary to E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_3_output\\analysis_summary_random.txt...\n",
      "  Analysis Summary TXT saved.\n",
      "  Full partition CSV saved to: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_3_output\\random_full_partition.csv\n",
      "  - Generating Top 5 Stacks summary table...\n",
      "  Top Stacks Summary CSV saved.\n",
      "  - [Vis Prep] Calculating PageRank (Top 100)...\n",
      "   Exported GEXF: 1_pagerank_core_for_gephi_random.gexf\n",
      "  - [Vis Prep] Extracting Top 5 largest communities...\n",
      "   Exported GEXF: 2_top_5_communities_for_gephi_random.gexf\n",
      "\n",
      "Loading random graph from: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_4.pkl\n",
      "Graph loaded. Nodes=397797, Edges=1819936\n",
      "\n",
      "==================== Analyzing: Random Graph: random_graph_4.pkl ====================\n",
      "  Optimization Complete. Q Score: 0.2388\n",
      "  - Total Communities Found: 507\n",
      "-> Q Score recorded: 0.2388\n",
      "  - Generating Analysis Summary to E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_4_output\\analysis_summary_random.txt...\n",
      "  Analysis Summary TXT saved.\n",
      "  Full partition CSV saved to: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_4_output\\random_full_partition.csv\n",
      "  - Generating Top 5 Stacks summary table...\n",
      "  Top Stacks Summary CSV saved.\n",
      "  - [Vis Prep] Calculating PageRank (Top 100)...\n",
      "   Exported GEXF: 1_pagerank_core_for_gephi_random.gexf\n",
      "  - [Vis Prep] Extracting Top 5 largest communities...\n",
      "   Exported GEXF: 2_top_5_communities_for_gephi_random.gexf\n",
      "\n",
      "Loading random graph from: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_5.pkl\n",
      "Graph loaded. Nodes=397797, Edges=1819936\n",
      "\n",
      "==================== Analyzing: Random Graph: random_graph_5.pkl ====================\n",
      "  Optimization Complete. Q Score: 0.2385\n",
      "  - Total Communities Found: 497\n",
      "-> Q Score recorded: 0.2385\n",
      "  - Generating Analysis Summary to E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_5_output\\analysis_summary_random.txt...\n",
      "  Analysis Summary TXT saved.\n",
      "  Full partition CSV saved to: E:\\Network Science Project\\01_Louvain_method\\RandomData\\random_graph_5_output\\random_full_partition.csv\n",
      "  - Generating Top 5 Stacks summary table...\n",
      "  Top Stacks Summary CSV saved.\n",
      "  - [Vis Prep] Calculating PageRank (Top 100)...\n",
      "   Exported GEXF: 1_pagerank_core_for_gephi_random.gexf\n",
      "  - [Vis Prep] Extracting Top 5 largest communities...\n",
      "   Exported GEXF: 2_top_5_communities_for_gephi_random.gexf\n",
      "\n",
      "==================================================\n",
      " Random Graph Baseline Summary\n",
      "Individual Q Scores: [0.23885048189572872, 0.2385925176304536, 0.23888069779567297, 0.23877546427005067, 0.2384912186462746]\n",
      "Average Modularity Q_rand: 0.2387\n",
      "Standard Deviation StdDev_rand: 0.0002\n",
      "Baseline comparison data saved to: E:\\Network Science Project\\01_Louvain_method\\RandomData\\baseline_q_comparison.json\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# (Objective5) 01_Louvain_RandomData\n",
    "import pickle\n",
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import community.community_louvain as community_louvain \n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION SECTION \n",
    "# ==========================================\n",
    "BASE_OUTPUT_PATH = r\"E:\\Network Science Project\\01_Louvain_method\\RandomData\"\n",
    "REAL_DATA_MODULARITY_Q = 0.4783 # Q value from real data\n",
    "\n",
    "# sample set\n",
    "TOP_COMMUNITIES_SAMPLE = 5\n",
    "PAGERANK_SAMPLE_NODES = 100 \n",
    "\n",
    "# ==========================================\n",
    "# 2. UTILITIES\n",
    "# ==========================================\n",
    "\n",
    "def load_random_graph(file_path: str) -> nx.DiGraph:\n",
    "    \"\"\"Loads a NetworkX graph object from a .pkl file.\"\"\"\n",
    "    print(f\"\\nLoading random graph from: {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\" Error: Random graph file NOT FOUND at {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            G_random = pickle.load(f)\n",
    "            if isinstance(G_random, nx.DiGraph) or isinstance(G_random, nx.Graph):\n",
    "                 print(f\"Graph loaded. Nodes={G_random.number_of_nodes()}, Edges={G_random.number_of_edges()}\")\n",
    "                 return G_random\n",
    "            else:\n",
    "                 raise TypeError(\"File content is not a NetworkX graph object.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading pickle file: {e}\")\n",
    "        raise\n",
    "\n",
    "def export_analysis_summary(G: nx.DiGraph, results: dict, output_path: str, filename: str = \"analysis_summary.txt\"):\n",
    "    \"\"\"\n",
    "    Exports all key summary data (Q, diagnostics, Top 10 Hubs) to a single TXT file.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(output_path, filename)\n",
    "    print(f\"  - Generating Analysis Summary to {file_path}...\")\n",
    "\n",
    "    #  Top 10 Stacks' Hubs \n",
    "    stacks_text = \"\\n\"\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    \n",
    "    top_comms = results.get('top_communities_summary', [])\n",
    "    \n",
    "    for i, (comm_id, nodes) in enumerate(top_comms[:10]):\n",
    "        top_hubs = sorted(nodes, key=lambda x: G.in_degree(x), reverse=True)[:8]\n",
    "        share = (len(nodes) / num_nodes) * 100\n",
    "        \n",
    "        stacks_text += f\"Stack #{i+1} (ID: {comm_id}): Size={len(nodes)} ({share:.2f}%)\\n\"\n",
    "        stacks_text += f\"  Core Hubs: {', '.join(top_hubs)}\\n\"\n",
    "    \n",
    "    summary_content = f\"\"\"\n",
    "==================================================\n",
    "        NETWORK ANALYSIS SUMMARY (RANDOM DATA)\n",
    "==================================================\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Graph Name: {results.get('graph_name', 'Unknown')}\n",
    "\n",
    "--- GRAPH DIAGNOSTICS ---\n",
    "Nodes (Packages): {G.number_of_nodes()}\n",
    "Edges (Dependencies): {G.number_of_edges()}\n",
    "DAG Property: {'Valid (No cycles)' if nx.is_directed_acyclic_graph(G) else 'Cycles Detected'}\n",
    "\n",
    "--- COMMUNITY STRUCTURE (Louvain) ---\n",
    "Modularity Score (Q): {results['modularity']:.4f}\n",
    "Total Communities Found: {results['num_communities']}\n",
    "\n",
    "--- TOP 10 LARGEST STACKS (RANDOM STRUCTURE) ---\n",
    "{stacks_text}\n",
    "==================================================\n",
    "\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_content)\n",
    "    print(f\"  Analysis Summary TXT saved.\")\n",
    "\n",
    "def export_partition_to_csv(partition: dict, output_path: str, filename: str = \"random_full_partition.csv\"):\n",
    "    \"\"\"Exports the complete node-to-community partition to a CSV file.\"\"\"\n",
    "    file_path = os.path.join(output_path, filename)\n",
    "    \n",
    "    df_partition = pd.DataFrame(\n",
    "        list(partition.items()), \n",
    "        columns=['Package', 'CommunityID']\n",
    "    )\n",
    "    df_partition.to_csv(file_path, index=False)\n",
    "    print(f\"  Full partition CSV saved to: {file_path}\")\n",
    "\n",
    "def export_top_stacks_table(G: nx.DiGraph, sorted_communities: list, top_k: int, output_path: str, filename: str):\n",
    "    \"\"\"Exports the core data for the Top K Stacks as a CSV table.\"\"\"\n",
    "    file_path = os.path.join(output_path, filename)\n",
    "    print(f\"  - Generating Top {top_k} Stacks summary table...\")\n",
    "    \n",
    "    table_data = []\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    \n",
    "    for i, (comm_id, nodes) in enumerate(sorted_communities[:top_k]):\n",
    "        top_hubs = sorted(nodes, key=lambda x: G.in_degree(x), reverse=True)[:8]\n",
    "        share = (len(nodes) / num_nodes) * 100\n",
    "        \n",
    "        table_data.append({\n",
    "            'Stack #': i + 1,\n",
    "            'Community ID': comm_id,\n",
    "            'Size (Nodes)': len(nodes),\n",
    "            'Share (%)': f'{share:.2f}',\n",
    "            'Core Packages (Hubs)': ', '.join(top_hubs)\n",
    "        })\n",
    "        \n",
    "    df_summary = pd.DataFrame(table_data)\n",
    "    df_summary.to_csv(file_path, index=False)\n",
    "    print(f\"  Top Stacks Summary CSV saved.\")\n",
    "\n",
    "def extract_core_subgraph(G: nx.DiGraph, partition: dict, num_nodes: int) -> tuple:\n",
    "    \"\"\"Extracts the core subgraph based on PageRank authority.\"\"\"\n",
    "    print(f\"  - [Vis Prep] Calculating PageRank (Top {num_nodes})...\")\n",
    "    pagerank_scores = nx.pagerank(G, alpha=0.85)\n",
    "    top_nodes = sorted(pagerank_scores, key=pagerank_scores.get, reverse=True)[:num_nodes]\n",
    "    \n",
    "    core_subgraph = G.subgraph(top_nodes).copy()\n",
    "    core_partition = {node: partition[node] for node in core_subgraph.nodes() if node in partition}\n",
    "    return core_subgraph, core_partition\n",
    "\n",
    "def extract_top_community_subgraph(G: nx.DiGraph, sorted_communities: list, top_k: int) -> nx.DiGraph:\n",
    "    \"\"\"Extracts the subgraph containing only the top K largest communities.\"\"\"\n",
    "    print(f\"  - [Vis Prep] Extracting Top {top_k} largest communities...\")\n",
    "    top_k_nodes = set()\n",
    "    for _, nodes in sorted_communities[:top_k]:\n",
    "        top_k_nodes.update(nodes)\n",
    "        \n",
    "    top_comm_subgraph = G.subgraph(top_k_nodes).copy()\n",
    "    return top_comm_subgraph\n",
    "\n",
    "def export_for_visualization(G: nx.DiGraph, partition: dict, output_path: str, filename: str):\n",
    "    \"\"\"Exports to Gephi (.gexf).\"\"\"\n",
    "    file_path = os.path.join(output_path, filename)\n",
    "    G_export = G.copy()\n",
    "    nx.set_node_attributes(G_export, partition, name='CommunityID')\n",
    "    \n",
    "    edge_type = {}\n",
    "    for u, v in G_export.edges():\n",
    "        u_comm = partition.get(u, -1)\n",
    "        v_comm = partition.get(v, -1)\n",
    "        if u_comm == v_comm and u_comm != -1:\n",
    "            edge_type[(u, v)] = 'Internal'\n",
    "        else:\n",
    "            edge_type[(u, v)] = 'Inter-Community'\n",
    "            \n",
    "    nx.set_edge_attributes(G_export, edge_type, name='LinkType')\n",
    "    nx.write_gexf(G_export, file_path)\n",
    "    print(f\"   Exported GEXF: {filename}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. CORE ANALYSIS LOGIC \n",
    "# ==========================================\n",
    "\n",
    "def analyze_community_structure(G: nx.DiGraph, graph_name: str = \"Unknown Graph\", top_k_stacks: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Performs community detection using Louvain algorithm.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Analyzing: {graph_name} {'='*20}\")\n",
    "    \n",
    "    # 1. Undirected Projection\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    # 2. Louvain Algorithm\n",
    "    try:\n",
    "        partition = community_louvain.best_partition(G_undirected)\n",
    "    except ValueError as e:\n",
    "        print(f\"  Error during Louvain execution: {e}\")\n",
    "        return {}\n",
    "\n",
    "    # 3. Modularity\n",
    "    modularity_score = community_louvain.modularity(partition, G_undirected)\n",
    "    print(f\"  Optimization Complete. Q Score: {modularity_score:.4f}\")\n",
    "    \n",
    "    # 4. Stats\n",
    "    community_map = defaultdict(list)\n",
    "    for node, comm_id in partition.items():\n",
    "        community_map[comm_id].append(node)\n",
    "    \n",
    "    sorted_communities = sorted(community_map.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    num_communities = len(sorted_communities)\n",
    "    print(f\"  - Total Communities Found: {num_communities}\")\n",
    "\n",
    "    return {\n",
    "        \"graph_name\": graph_name,\n",
    "        \"modularity\": modularity_score,\n",
    "        \"num_communities\": num_communities,\n",
    "        \"partition\": partition,\n",
    "        \"top_communities_summary\": sorted_communities \n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    random_files = [f\"random_graph_{i}.pkl\" for i in range(1, 6)]\n",
    "    all_modularity_scores = []\n",
    "    \n",
    "    print(\"--- Starting Random Graph Baseline Analysis (Objective 5) ---\")\n",
    "    \n",
    "    for filename in random_files:\n",
    "        # --- 1. path set and outout file set ---\n",
    "        folder_name = filename.replace('.pkl', '_output')\n",
    "        output_dir = os.path.join(BASE_OUTPUT_PATH, folder_name)\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # print(f\"\\nCreated output directory: {output_dir}\")\n",
    "        \n",
    "        full_path = os.path.join(BASE_OUTPUT_PATH, filename) \n",
    "        \n",
    "        try:\n",
    "            # 2. loading random graph\n",
    "            G_random = load_random_graph(full_path)\n",
    "\n",
    "            # 3. run function 1\n",
    "            random_results = analyze_community_structure(\n",
    "                G_random, \n",
    "                graph_name=f\"Random Graph: {filename}\",\n",
    "                top_k_stacks=10 \n",
    "            )\n",
    "            \n",
    "            partition = random_results.get('partition')\n",
    "            Q_score = random_results.get('modularity')\n",
    "            \n",
    "            if partition and Q_score is not None:\n",
    "                all_modularity_scores.append(Q_score)\n",
    "                print(f\"-> Q Score recorded: {Q_score:.4f}\")\n",
    "\n",
    "                # --- 4. Export ---\n",
    "\n",
    "                # A. Export summary TXT\n",
    "                export_analysis_summary(\n",
    "                    G_random, \n",
    "                    random_results, \n",
    "                    output_dir, \n",
    "                    filename=\"analysis_summary_random.txt\"\n",
    "                )\n",
    "                \n",
    "                # B. Export full partition CSV\n",
    "                export_partition_to_csv(\n",
    "                    partition, \n",
    "                    output_dir, \n",
    "                    filename=\"random_full_partition.csv\"\n",
    "                )\n",
    "                \n",
    "                # C. Export Top 5 Stacks CSV\n",
    "                export_top_stacks_table(\n",
    "                    G_random, \n",
    "                    random_results['top_communities_summary'], \n",
    "                    TOP_COMMUNITIES_SAMPLE, \n",
    "                    output_dir, \n",
    "                    filename=\"top_5_stacks_summary_random.csv\"\n",
    "                )\n",
    "\n",
    "                # D. Export PageRank core subgraph GEXF\n",
    "                core_G, core_partition = extract_core_subgraph(\n",
    "                    G_random, \n",
    "                    partition, \n",
    "                    num_nodes=PAGERANK_SAMPLE_NODES\n",
    "                )\n",
    "                export_for_visualization(\n",
    "                    core_G, \n",
    "                    core_partition, \n",
    "                    output_dir, \n",
    "                    filename=\"1_pagerank_core_for_gephi_random.gexf\"\n",
    "                )\n",
    "\n",
    "                # E. Export Top 5 community subgraph GEXF\n",
    "                top_comm_G = extract_top_community_subgraph(\n",
    "                    G_random, \n",
    "                    random_results['top_communities_summary'], \n",
    "                    top_k=TOP_COMMUNITIES_SAMPLE\n",
    "                )\n",
    "                # proj partition\n",
    "                top_comm_partition = {node: partition[node] for node in top_comm_G.nodes() if node in partition}\n",
    "\n",
    "                export_for_visualization(\n",
    "                    top_comm_G, \n",
    "                    top_comm_partition, \n",
    "                    output_dir, \n",
    "                    filename=\"2_top_5_communities_for_gephi_random.gexf\"\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                print(f\"--- Analysis FAILED for {filename} (Modularity is None) ---\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n--- Analysis FAILED for {filename} ---\")\n",
    "            print(f\"Error encountered: {e}\")\n",
    "            continue\n",
    "\n",
    "    # --- 5. summ JSON ---\n",
    "    if all_modularity_scores:\n",
    "        avg_q = np.mean(all_modularity_scores)\n",
    "        std_q = np.std(all_modularity_scores)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\" Random Graph Baseline Summary\")\n",
    "        print(f\"Individual Q Scores: {all_modularity_scores}\")\n",
    "        print(f\"Average Modularity Q_rand: {avg_q:.4f}\")\n",
    "        print(f\"Standard Deviation StdDev_rand: {std_q:.4f}\")\n",
    "        \n",
    "        baseline_summary = {\n",
    "            'Q_real': REAL_DATA_MODULARITY_Q,\n",
    "            'Q_rand_avg': avg_q,\n",
    "            'Q_rand_std': std_q,\n",
    "            'Q_rand_individual': all_modularity_scores\n",
    "        }\n",
    "        output_file = os.path.join(BASE_OUTPUT_PATH, 'baseline_q_comparison.json')\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(baseline_summary, f, indent=4)\n",
    "        \n",
    "        print(f\"Baseline comparison data saved to: {output_file}\")\n",
    "        print(\"=\"*50)\n",
    "    else:\n",
    "        print(\" No random graphs were successfully analyzed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
